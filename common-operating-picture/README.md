# Virtru Data Security Platform - Common Operational Picture

> [!WARNING]
> Virtru's DSP COP is a demonstration application and is not intended for production use out-of-the box.
> The current iteration is **in active development** and is not intended to be a fully-featured application.
>
> If you have received a copy of this application from Virtru, it is provided for demonstration purposes only.

The DSP COP is an application to demonstrate the following:

1. [Data-Centric Security](https://www.virtru.com/data-security-platform) (ABAC, ZTDF)
2. Schema-driven User Interfaces to drive TDF creation
3. Secure, platform-protected search of TDF'd data

Table of Contents

1. [Quick Start](#quick-start)
2. [Development and Contribution](#development-and-contribution)
3. [Run Locally](#run-locally)
4. [Production](#production)
5. [Schema Creation](#schema-creation)
6. [Known Issues](#known-issues)

## Quick Start

This application can be run in two modes:
1. `Lite` - Sits on top of an already-deployed DSP stack
   - Follow [lite-deployment.md](./deploy/lite/lite-deployment.md)
1. `Full` - Deploys a full DSP stack
   - Continue with this Readme.

## Development and Contribution

### Server

COP is powered by a backend built with the following technologies:

1. [Golang](https://go.dev/), with the binary containing two commands

   * `start` to [run the server](./cmd/startCmd.go)
   * `db` for enhanced development experience interacting [directly with the database, including mocks](./cmd/dbCmd.go)

2. [Connect](https://buf.build/blog/connect-a-better-grpc)

   * compatible with [gRPC](https://grpc.io/), HTTP, and [ConnectRPC](https://connectrpc.com/)
   * driven by proto definitions and protobufs (see [proto definition file](./proto/tdf_object/v1/tdf_object.proto))
   * generated by [buf](https://buf.build/) (see [buf.gen.yaml](./buf.gen.yaml) for Go code generation from protos)
   * more information available in [the protos section](#protos)

3. [PostgreSQL](https://www.postgresql.org/) with [schema diagram](./db/entityRelationshipDiagram.md)

   * enhanced geometry types provided by [PostGIS extension](https://postgis.net/)
   * schema management provided by [atlas](https://atlasgo.io/)
   * db interactivity driven by [sqlc](https://github.com/sqlc-dev/sqlc) (see [sqlc.yaml](./sqlc.yaml) and filed marked
    generated within [/db](./db/))

In order to serve both static files (frontend COP) and the API interactivity with the COP database, two ports are opened
on the COP Go server. These ports are configured within the `config.yaml` config file ([see example](./config.example.yaml)
and [Configuration](#configuration)).

### Configuration

The configuration file is mapped in `config.example.yaml` and can be located at specific locations
scoped to how you are running the application.

1. Project-level: `/config.docker.yaml/config.docker.yaml`
2. User-level: `~/.dsp-cop/config.yaml`
3. System-level: `/etc/dsp-cop/config.yaml`

> [!NOTE]
> The DSP config (`dsp.yaml`) can be found and updated in the root-level of this project. It was previously embedded in `dev.dsp.Dockerfile`.

#### New Configuration Options

If you add a new configuration option, please update the `config.example.yaml` file and document the
new option in the `/pkg/config/config.go` file as well as comments in the example file.

### Protos

Our native gRPC service functions are generated from `proto` definitions using [Buf](https://buf.build/docs/introduction).

The `Makefile` provides command scripts to invoke `Buf` with the `buf.gen.yaml` config, including server Go code and browser TypeScript (JavaScript-compatible) code.

For convenience, the `make toolcheck` script checks if you have the necessary dependencies for `proto -> gRPC` generation.

## Run Locally

To demonstrate the capabilities of DSP, there are two root-level files to provision the platform.

1. [`sample.federal_policy.yaml` DSP Policy](./sample.federal_policy.yaml)
    * Attributes (a Namespace, Definitions, Values)
    * Subject Mappings to entitle users and clients
2. [`sample.keycloak.yaml` Users and Clients](./sample.keycloak.yaml) to be loaded into Keycloak as the demo identityProvider (idP)

> [!NOTE]
> PostGIS uses a [community-maintained image](https://github.com/ImreSamu/docker-postgis#debian---bookworm--recommended)
> for compatibility.
> There is [an open issue](https://github.com/postgis/docker-postgis/issues/371) with the official image running on `amd64` architecture.

### Pre-requisites

1. **Run the Setup Script**
To install necessary dependencies automatically, run the provided script:

   ```bash
   ./ubuntu_cop_prereqs_cop.sh

   # Reboot after running script for some changes to take effect
   reboot
   ```

   <details>
   <summary><strong>Manual Installation Details (Optional)</strong></summary>

   If you prefer to install manually or need to debug, the script handles the following:

   - **Container Runtime:** Installs Docker + Docker Compose.
     - _Alternatives supported:_ [Colima (recommended)](https://github.com/abiosoft/colima), [Rancher Desktop](https://rancherdesktop.io), or [Podman Desktop](https://podman-desktop.io).
   - **Languages & Tools:**
     - [Node.js (via nvm)](https://nodejs.org/en/download/package-manager)
     - [Go (Golang)](https://go.dev/doc/install)
     - [GEOS](https://libgeos.org/usage/install/)
     - [Make](https://formulae.brew.sh/formula/make)
     </details>
   - **Local DNS Configuration**
     - Entry added into /etc/hosts
     - ```text
       127.0.0.1    local-dsp.virtru.com
       ```

---

### Step 1: Generate Local Certificates (Mkcert)

You need SSL certificates for local development.

**Option A: Script Setup**
Run the key generation script:

```bash
./ubuntu_cop_keys.sh
```

**Option B: Make Command**
** Note: you can use `'make dev-certs'` as a shortcut to generate the development certs **

**Currently NonFunctional - Use the script above**

```bash
# Make command
make dev-certs
```

### Step 2: Unpack the Bundle

Unzip the main bundle and unpack the specific DSP tools. Replace `X.X.X`, `<os>`, and `<arch>` with your specific version and system details.

   ```bash
   # 1. Untar the main bundle
   mkdir virtru-dsp-bundle && tar -xvf virtru-dsp-bundle-* -C virtru-dsp-bundle/ && cd virtru-dsp-bundle/

   # 2. Unpack DSP Tools
   tar -xvf tools/dsp/data-security-platform_X.X.X_<os>_<arch>.tar.gz
      #Example - AMD linux:
      tar -xvf tools/dsp/data-security-platform_2.7.1_linux_amd64.tar.gz

   # 3. Unpack and setup Helm
   tar -xvf tools/helm/helm-vX.X.X-<os>-<arch>.tar.gz
      #Example - AMD linux:
      tar -xvf tools/helm/helm-v3.15.4-linux-amd64.tar.gz
   # Then move command into working directory
   mv <os>-<arch>/helm ./helm

   # 4. Unpack and setup grpcurl
   tar -xvf tools/grpcurl/grpcurl_X.X.X_<os>_<arch>.tar.gz
      #Example - AMD linux:
      tar -xvf tools/grpcurl/grpcurl_1.9.1_linux_x86_64.tar.gz

   # Make Executable
   chmod +x ./grpcurl
   ```

### Step 3: Setup Local Docker Registry

The DSP images are stored in the bundle as OCI artifacts. You must spin up a local registry and copy the images into it.

   ```bash
   # 1. Start a local registry instance
   docker run -d --restart=always -p 5000:5000 --name registry registry:2

   # 2. Copy DSP images into local registry
   # (Run this from the virtru-dsp-bundle root directory)
   ./dsp copy-images --insecure localhost:5000/virtru

   # 3. Verify images were copied successfully
   curl -X GET http://localhost:5000/v2/_catalog
   curl -X GET http://localhost:5000/v2/virtru/data-security-platform/tags/list
   ```

### Step 4: Build and Run

Use Docker Compose to build and start the environment.

**Start the environment:**

```bash
docker compose --env-file env/default.env -f docker-compose.dev.yaml up --build --force-recreate
```
   This starts the following services:

   1. [COP database](./compose/docker-compose.cop-db.yaml)
   2. [Keycloak](./compose/docker-compose.keycloak.yaml)
   3. [Data Security Platform (DSP)](./compose/docker-compose.dsp.yaml)
      1. Provision keycloak with sample users and clients
      2. Setup the database
      3. Start the DSP server
      4. Load the sample policy
   4. [COP Web Server](./compose/docker-compose.cop-web-server.yaml)
   5. [NiFi](./compose/docker-compose.nifi.yaml) (_disabled by default_)
      > [!NOTE]
      > Nifi is resource-intensive, so you should run `colima` with extra resources allocated: `colima start --memory 16 --cpu 6`
      1. For local docker compose, run the [build_truststore_local.sh](./build_truststore_local.sh)  to build a truststore for use with NiFi and Tagging Services
      2. Copy the trusted cert for tagging pdp use to it's mounted drive: `cp ./dsp-keys/local-dsp.virtru.com.pem ./nifi/truststore`
      3. Run with envfile and nifi profile enabled: `docker compose --profile nifi -f docker-compose.all.yaml --env-file=./env/default.env up`
         * Note that NiFi uses significant resources; ensure your docker env has sufficient resources allocated

Local COP Application URL: https://local-dsp.virtru.com:5001/

**Stop the environment:**

The following will stop the enviroment and COP application. Crtl + c in the terminal will also stop the containers however it is recommended
to also run the following down command as it will cleanup the container remnants.

```bash
docker compose --env-file env/default.env -f docker-compose.dev.yaml down
```

---

### Step 5. Seeding Vehicle Data and Live Data Flow Simulation

Following the successful building of COP:

   ```bash
   # Install the venv module
   sudo apt install python3-venv -y

   # Create a virtual environment named 'COP_venv' in the current directory
   python3 -m venv COP_venv
   ```

   ```bash
   # Activate the virtual environment.
   # Your shell prompt will change to indicate it's active.
   source COP_venv/bin/activate
   ```

   ```bash
   # Pip install all required package from requirements.txt
   pip install -r requirements.txt
   ```

   ```bash
   # Run seeding script to populate database
   # 50 is the standard number of objects that the script will inset but is configurable via NUM_RECORDS variable
   python3 seed_data.py
   ```

   ```bash
   # Start simulation
   # NUM_ENTITIES will determine how many moving entities the script will query the database for and apply movement logic to
   # UPDATE_INTERVAL_SECONDS determins the frequency of movement for each object
   # BOUNDING_BOX_PARAMS define the area for the OpenSky query for live planes (smaller box results in less credits used on init).

   # For live data from OpenSky Network login to https://opensky-network.org/, download credentials file (credentials.json),
   # place the file in the base director (where the sim_data.py script is located) and then run:
   python3 sim_data.py

   # For a fake simulation that does not require the credentials file or use account credits with OpenSky run this script
   # for simulated movement:
   python3 sim_data_fake_opensky.py
   ```

### Troubleshooting & Verification Checklist

If you encounter issues, double-check the following:

- **dsp.yaml:** Ensure this file exists in your working directory.
- **rootCA.cert:** Ensure the root CA certificate was copied correctly during the setup.
- **Permissions:** Verify that the certificates in `dsp-keys` have `chmod 755` permissions.

### Run the COP Server Locally

> [!NOTE]
> When running the COP Server locally, use `docker-compose.dev.yaml` instead of `docker-compose.all.yaml` to omit the cop-web-server from the Docker environment.
> To run without mocked static assets, must have followed [frontend README](/ui/README.md).

```bash
# Run server with mock frontend assets
go run . serve
```

#### Run with static assets

To run the server with the statically built COP frontend. (See [ui README](/ui/README.md) for more information.)

```bash
go run -tags embedfiles . serve
```

### Web UI

Run the web UI in development mode

```shell
# in context of web ui
cd ui
# copy the .env.example to .env
cp .env.example .env
# switch to required node engine
nvm use
# install app dependencies
npm ci
# start the development server
npm start
```

See the [ui README](/ui/README.md) for more information on the frontend development environment including **running statically built assets.**

### Authentication

DSP-COP supports two authentication flows:
1. Username/Password (default)
   - user enters credentials into application form, application initiates token request
2. Keycloak Authentication (PKI/x509 or username/password)
   - Application redirects user to Keycloak for authentication, keycloak returns a token

To enable the Keycloak authentication flow, set the following value in the `ui/.env`
   - `VITE_DSP_KC_DIRECT_AUTH=true`

To enable and configure x509 based login, see `x509.md`

---

## Production

> [!NOTE]
> When running in production you will most likely need to set the `service.public_server_host` and the `service.public_static_host` to the appropriate domain and port. This is required when the server is running behind a reverse proxy.

*Details on running in production is not yet available.*

## Schema Creation

Within COP, we refer to the individual data entities used by the system as "Source Types". Each Source Type has a configuration that defines the structure and behavior of the data ("form schema"), how the data is displayed and interacted with from the UI ("UI schema"), and additional information on how this data entity is used by the application ("metadata"). These configurations are stored in the database and are used to generate the forms and UI elements that users interact with when creating and viewing data in COP.

### Database Structure

Source Type configurations are stored in the `src_types` table of the database.  Each column besides the unique `id` uses the JSONB data type to store the schema configuration as JSON data. The columns are as follows (see the [entity relationship diagram](./db/entityRelationshipDiagram.md) for more details):

```postgresql
id TEXT
form_schema JSONB
ui_schema JSONB
metadata JSONB
```

### Creating a new Source Type

At the time of this writing, Source Type creation is a manual process using a SQL seed data script that is run when composing up with docker. This seed script can be found at [db/seed.sql](db/seed.sql) and can be modified to include as many source types as desired. In the future, this may be replaced with YAML files that are easier to read/maintain, or even managed within the COP application directly from an admin page.

To create the JSON data stored by each column, use your favorite JSON text editor and then serialize the JSON to a string using an online tool or your preferred language's library (e.g. `JSON.stringify` with JavaScript). Once you have the string, add it to the seed script in the appropriate column.

Two different source types are currently pre-defined in the seed script: `sitrep` and `employee`.  Please refer to these examples for further guidance beyond this documentation when creating the JSONB values for each column.

#### Defining the Form Schema

The Form Schema is defined using the [JSON Schema specification](https://json-schema.org/). This schema defines the structure of the data for the Source Type. The schema is later used with the UI to generate the form fields for entering data, as well as validate the data entered by the user in those form fields.

COP uses the [react-jsonschema-form](https://rjsf-team.github.io/react-jsonschema-form/docs/) library for generating these forms.

#### Defining the UI Schema

The UI Schema is defined using a standard JSON object, but its structure is coupled to the React library mentioned above for form generation. Thus, the format of the JSON object here must follow the defined [UI Schema API structure](https://rjsf-team.github.io/react-jsonschema-form/docs/api-reference/uiSchema).

To learn more about how the Form and UI Schemas work together, visit the [React JSON Schema Form Playground](https://rjsf-team.github.io/react-jsonschema-form/). You can experiment with different JSON and UI Schema configurations to see how they affect the form rendering and behavior.

#### Defining the Metadata

The Metadata is defined using a standard JSON object with a known structure of key/value pairs. It can evolve as requirements change, but the current structure is as follows:

`geoField` _string_ - The name of the form schema field that should be used for location data when writing to the `tdf_objects.geo` database table column.

`searchFields` _string[]_ - An array of form schema field names that should be used when writing JSONB searchable key/value pairs in unencrypted plaintext to the `tdf_objects.search` database table column.

`attrFields` _string[]_ - An array of form schema field names that define the fields used for access control when encrypting data as a TDF to be stored in the `tdf_objects.tdf_blob` database table column.

`tsField` _string_ - The name of the form schema field that should be used for the timestamp when writing to the `tdf_objects.ts` database table column.  If not defined, the database will use the current timestamp by default.

`displayFields` _object_ - An object with two keys: `header` and `details`. The `header` key is a string that defines the form schema field name to be used as the header when displaying the data in the UI. The `details` key is an array of strings that define the form schema field names to be used as the detail list when displaying data in the UI.

`mapFields` _object_ - An object with four keys: `iconDefault`, `iconConfig`, `colorDefault` and `colorConfig`. This object defines the form schema field names to be used when customizing the map marker icon and color. The `iconDefault` and `colorDefault` keys define the default icon and color mapping to be used when no other mapping is defined. The `iconConfig` and `colorConfig` keys are arrays of objects that define the field names and values to be used for mapping those field values to the proper icons and colors within the COP UI.

#### Using `dsp` and `dsp tructl` for development

After provisioning the developer's local stack, the following workflow
may be used to interact with the `dsp` and `dsp tructl` CLIs.

```bash
# Open a shell
make launch-dsp-shell

# Once inside, access tructl functionality
dsp tructl --with-client-creds '{"clientId":"opentdf","clientSecret":"secret"}' policy attributes list --host https://local-dsp.virtru.com:8080 --tls-no-verify
```

## Known Issues

* Update create RPC handler returns an empty UUID if the insert fails due to a failure to connect to DB
* SITREP form CSS and asterisks for required fields do not match validation (which contains the real requirements logic)
* Possible to zoom out the map and see countries multiple times with only single pins rendered
* Autocomplete in form UI is unintuitive
* When searching, no UI validation of start and end date/time
   * start date is required
   * start date must be before end date
* URL search params are not cleared when swithing between source types, resulting in unrelated params breaking search
* Source Type is NULLable and should be required
* Passing a non-existent source type for the `type` URL param in the SourceTypes component, breaks the selector and page components
